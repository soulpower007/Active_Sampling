{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Set the `numpy` pseudo-random generator at a fixed value\n",
    "#This helps with repeatable results everytime you run the code. \n",
    "np.random.seed(1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow' # Added to set the backend as Tensorflow\n",
    "#We can also set it to Theano if we want. \n",
    "\n",
    "\n",
    "#Iterate through all images in Parasitized folder, resize to 64 x 64\n",
    "#Then save as numpy array with name 'dataset'\n",
    "#Set the label to this as 0\n",
    "\n",
    "image_directory = 'cell_images/'\n",
    "SIZE = 64\n",
    "dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.  \n",
    "label = []  #Place holders to define add labels. We will add 0 to all parasitized images and 1 to uninfected.\n",
    "\n",
    "parasitized_images = os.listdir(image_directory + 'Parasitized/')\n",
    "\n",
    "for i, image_name in enumerate(parasitized_images):    #Remember enumerate method adds a counter and returns the enumerate object\n",
    "    \n",
    "    if (image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(image_directory + 'Parasitized/' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# # #full path.\n",
    "# # dir_path = os.path.dirname(os.path.realpath())\n",
    "# # print(dir_path)\n",
    "# #current dir.\n",
    "# cwd = os.getcwd()\n",
    "# print(cwd)\n",
    "\n",
    "# print(os.listdir('/home/system11-user3/Desktop/AS/cell_images/Parasitized/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 4,340,130\n",
      "Trainable params: 4,338,466\n",
      "Non-trainable params: 1,664\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Iterate through all images in Uninfected folder, resize to 64 x 64\n",
    "#Then save into the same numpy array 'dataset' but with label 1\n",
    "\n",
    "uninfected_images = os.listdir(image_directory + 'Uninfected/')\n",
    "for i, image_name in enumerate(uninfected_images):\n",
    "    if (image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(image_directory + 'Uninfected/' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(1)\n",
    "\n",
    "\n",
    "#Apply CNN\n",
    "# ### Build the model\n",
    "\n",
    "#############################################################\n",
    "###2 conv and pool layers. with some normalization and drops in between.\n",
    "\n",
    "INPUT_SHAPE = (SIZE, SIZE, 3)   #change to (SIZE, SIZE, 3)\n",
    "inp = keras.layers.Input(shape=INPUT_SHAPE)\n",
    "\n",
    "conv1 = keras.layers.Conv2D(32, kernel_size=(3, 3), \n",
    "                               activation='relu', padding='same')(inp)\n",
    "pool1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "norm1 = keras.layers.BatchNormalization(axis = -1)(pool1)\n",
    "drop1 = keras.layers.Dropout(rate=0.2)(norm1)\n",
    "conv2 = keras.layers.Conv2D(32, kernel_size=(3, 3), \n",
    "                               activation='relu', padding='same')(drop1)\n",
    "pool2 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "norm2 = keras.layers.BatchNormalization(axis = -1)(pool2)\n",
    "drop2 = keras.layers.Dropout(rate=0.2)(norm2)\n",
    "\n",
    "flat = keras.layers.Flatten()(drop2)  #Flatten the matrix to get it ready for dense.\n",
    "\n",
    "hidden1 = keras.layers.Dense(512, activation='relu')(flat)\n",
    "norm3 = keras.layers.BatchNormalization(axis = -1)(hidden1)\n",
    "drop3 = keras.layers.Dropout(rate=0.2)(norm3)\n",
    "hidden2 = keras.layers.Dense(256, activation='relu')(drop3)\n",
    "norm4 = keras.layers.BatchNormalization(axis = -1)(hidden2)\n",
    "drop4 = keras.layers.Dropout(rate=0.2)(norm4)\n",
    "\n",
    "out = keras.layers.Dense(2, activation='sigmoid')(drop4)   #units=1 gives error\n",
    "\n",
    "model = keras.Model(inputs=inp, outputs=out)\n",
    "model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',   #Check between binary_crossentropy and categorical_crossentropy\n",
    "                metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "###############################################################    \n",
    "    \n",
    " ### Split the dataset\n",
    "# \n",
    "# I split the dataset into training and testing dataset.\n",
    "# 1. Training data: 80%\n",
    "# 2. Testing data: 20%\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, to_categorical(np.array(label)), test_size = 0.20, random_state = 0)\n",
    "\n",
    "# When training with Keras's Model.fit(), adding the tf.keras.callback.TensorBoard callback \n",
    "# ensures that logs are created and stored. Additionally, enable histogram computation \n",
    "#every epoch with histogram_freq=1 (this is off by default)\n",
    "#Place the logs in a timestamped subdirectory to allow easy selection of different training runs.\n",
    "\n",
    "#import datetime\n",
    "\n",
    "#log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"/\"\n",
    "#tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "# ### Training the model\n",
    "# As the training data is now ready, I will use it to train the model.   \n",
    "\n",
    "#Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X\n",
    "# Y_train = Y\n",
    "# X_test = X\n",
    "# Y_test = Y\n",
    "from random import random\n",
    "def eval_ratios(sx_train_g, sx_train_b, y_train_g, y_train_b, model):\n",
    "    test_g = model.test_on_batch(sx_train_g,y_train_g)\n",
    "    test_b = model.test_on_batch(sx_train_b,y_train_b)\n",
    "    negl = random()\n",
    "    if negl < 0.9: \n",
    "        return 1-test_g[1] + 0.4, 1-test_b[1] + 0.4 \n",
    "    else:\n",
    "        return random(), random()\n",
    "    \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "[0. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "scaled_x_train = np.array( X_train)\n",
    "# scaled_x_train = np.asarray(scaled_x_train).astype('float32')\n",
    "\n",
    "y_train = np.array(y_train).astype('float32')\n",
    "\n",
    "scaled_x_test = np.array(X_test)\n",
    "scaled_x_test = np.asarray(scaled_x_test).astype('float32')\n",
    "\n",
    "y_test = np.array(y_test).astype('float32')\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sx_train_g = scaled_x_train[ y_train.T[0]==1 ]\n",
    "sx_train_b = scaled_x_train[ y_train.T[1]==1 ]\n",
    "y_train_g = y_train[ y_train.T[0]==1.0 ]\n",
    "y_train_b = y_train[ y_train.T[1]==1.0 ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sx_train_g[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11076\n"
     ]
    }
   ],
   "source": [
    "print(len(sx_train_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Start:  2022-05-10 12:12:04.259412\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(\" Start: \", datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________ 0 0.5097968\n",
      "________________ 1 0.50961536\n",
      "________________ 2 0.51342523\n",
      "________________ 3 0.5883527\n",
      "________________ 4 0.5676705\n",
      "________________ 5 0.49709725\n",
      "________________ 6 0.49056605\n",
      "________________ 7 0.4902032\n",
      "________________ 8 0.4903846\n",
      "________________ 9 0.4903846\n",
      "________________ 10 0.49056605\n",
      "________________ 11 0.4903846\n",
      "________________ 12 0.4909289\n",
      "________________ 13 0.49129173\n",
      "________________ 14 0.51560235\n",
      "________________ 15 0.5569666\n",
      "________________ 16 0.5562409\n",
      "________________ 17 0.5576923\n",
      "________________ 18 0.56222785\n",
      "________________ 19 0.5703919\n",
      "________________ 20 0.58581275\n",
      "________________ 21 0.5821843\n",
      "________________ 22 0.57529026\n",
      "________________ 23 0.5660377\n",
      "________________ 24 0.55805516\n",
      "________________ 25 0.55079824\n",
      "________________ 26 0.54100144\n",
      "________________ 27 0.53610307\n",
      "________________ 28 0.5330189\n",
      "________________ 29 0.5324746\n",
      "________________ 30 0.532656\n",
      "________________ 31 0.53610307\n",
      "________________ 32 0.53846157\n",
      "________________ 33 0.54698837\n",
      "________________ 34 0.554971\n",
      "________________ 35 0.56313497\n",
      "________________ 36 0.5769231\n",
      "________________ 37 0.5867199\n",
      "________________ 38 0.5934325\n",
      "________________ 39 0.5990566\n",
      "________________ 40 0.6101234\n",
      "________________ 41 0.6171988\n",
      "________________ 42 0.62681425\n",
      "________________ 43 0.6367925\n",
      "________________ 44 0.6418723\n",
      "________________ 45 0.66182876\n",
      "________________ 46 0.6587446\n",
      "________________ 47 0.64985484\n",
      "________________ 48 0.6253629\n",
      "________________ 49 0.61338896\n",
      "________________ 50 0.59887516\n",
      "________________ 51 0.5941582\n",
      "________________ 52 0.5872642\n",
      "________________ 53 0.5845428\n",
      "________________ 54 0.58291\n",
      "________________ 55 0.5769231\n",
      "________________ 56 0.5683962\n",
      "________________ 57 0.55986935\n",
      "________________ 58 0.55370104\n",
      "________________ 59 0.54970974\n",
      "________________ 60 0.55079824\n",
      "________________ 61 0.5544267\n",
      "________________ 62 0.5689405\n",
      "________________ 63 0.58309144\n",
      "________________ 64 0.5883527\n",
      "________________ 65 0.5916183\n",
      "________________ 66 0.5974238\n",
      "________________ 67 0.6021408\n",
      "________________ 68 0.6034107\n",
      "________________ 69 0.60268503\n",
      "________________ 70 0.60867196\n",
      "________________ 71 0.6222787\n",
      "________________ 72 0.6306241\n",
      "________________ 73 0.6415094\n",
      "________________ 74 0.64840347\n",
      "________________ 75 0.64677066\n",
      "________________ 76 0.6485849\n",
      "________________ 77 0.6543904\n",
      "________________ 78 0.65457183\n",
      "________________ 79 0.64731497\n",
      "________________ 80 0.63661104\n",
      "________________ 81 0.63751817\n",
      "________________ 82 0.6362482\n",
      "________________ 83 0.64876634\n",
      "________________ 84 0.6565675\n",
      "________________ 85 0.65312046\n",
      "________________ 86 0.6317126\n",
      "________________ 87 0.6166546\n",
      "________________ 88 0.6068578\n",
      "________________ 89 0.5963353\n",
      "________________ 90 0.5927068\n",
      "________________ 91 0.5903483\n",
      "________________ 92 0.5907112\n",
      "________________ 93 0.5932511\n",
      "________________ 94 0.5896226\n",
      "________________ 95 0.5950653\n",
      "________________ 96 0.59379536\n",
      "________________ 97 0.60123366\n",
      "________________ 98 0.6063135\n",
      "________________ 99 0.615566\n",
      "________________ 100 0.620283\n",
      "________________ 101 0.620283\n",
      "________________ 102 0.62880987\n",
      "________________ 103 0.6387881\n",
      "________________ 104 0.64495647\n",
      "________________ 105 0.63824385\n",
      "________________ 106 0.63225687\n",
      "________________ 107 0.6407837\n",
      "________________ 108 0.6533019\n",
      "________________ 109 0.6563861\n",
      "________________ 110 0.6572932\n",
      "________________ 111 0.65820026\n",
      "________________ 112 0.6371553\n",
      "________________ 113 0.61175615\n",
      "________________ 114 0.59452105\n",
      "________________ 115 0.5932511\n",
      "________________ 116 0.58744556\n",
      "________________ 117 0.5838171\n",
      "________________ 118 0.58780843\n",
      "________________ 119 0.5981495\n",
      "________________ 120 0.62318575\n",
      "________________ 121 0.6469521\n",
      "________________ 122 0.66418725\n",
      "________________ 123 0.64241654\n",
      "________________ 124 0.5990566\n",
      "________________ 125 0.5774673\n",
      "________________ 126 0.56513065\n",
      "________________ 127 0.5609579\n",
      "________________ 128 0.5562409\n",
      "________________ 129 0.56113935\n",
      "________________ 130 0.55206823\n",
      "________________ 131 0.5495283\n",
      "________________ 132 0.54426706\n",
      "________________ 133 0.5446299\n",
      "________________ 134 0.5446299\n",
      "________________ 135 0.5464441\n",
      "________________ 136 0.55079824\n",
      "________________ 137 0.5544267\n",
      "________________ 138 0.5578737\n",
      "________________ 139 0.5694848\n",
      "________________ 140 0.5683962\n",
      "________________ 141 0.570029\n",
      "________________ 142 0.5660377\n",
      "________________ 143 0.5680334\n",
      "________________ 144 0.574746\n",
      "________________ 145 0.5814586\n",
      "________________ 146 0.58944124\n",
      "________________ 147 0.62681425\n",
      "________________ 148 0.66491294\n",
      "________________ 149 0.6857765\n",
      "________________ 150 0.6937591\n",
      "________________ 151 0.690312\n",
      "________________ 152 0.68414366\n",
      "________________ 153 0.66110307\n",
      "________________ 154 0.6380624\n",
      "________________ 155 0.6333454\n",
      "________________ 156 0.62481856\n",
      "________________ 157 0.6044993\n",
      "________________ 158 0.5925254\n",
      "________________ 159 0.5959724\n",
      "________________ 160 0.6014151\n",
      "________________ 161 0.6023222\n",
      "________________ 162 0.6041364\n",
      "________________ 163 0.6041364\n",
      "________________ 164 0.6159289\n",
      "________________ 165 0.64060235\n",
      "________________ 166 0.6835994\n",
      "________________ 167 0.7162554\n",
      "________________ 168 0.7249637\n",
      "________________ 169 0.7247823\n",
      "________________ 170 0.71317124\n",
      "________________ 171 0.6714441\n",
      "________________ 172 0.6357039\n",
      "________________ 173 0.60159653\n",
      "________________ 174 0.5908926\n",
      "________________ 175 0.58472425\n",
      "________________ 176 0.58581275\n",
      "________________ 177 0.5907112\n",
      "________________ 178 0.59452105\n",
      "________________ 179 0.6021408\n",
      "________________ 180 0.6068578\n",
      "________________ 181 0.61738026\n",
      "________________ 182 0.6191945\n",
      "________________ 183 0.625\n",
      "________________ 184 0.6347968\n",
      "________________ 185 0.66110307\n",
      "________________ 186 0.6850508\n",
      "________________ 187 0.7086357\n",
      "________________ 188 0.7298621\n",
      "________________ 189 0.7213353\n",
      "________________ 190 0.71389693\n",
      "________________ 191 0.6984761\n",
      "________________ 192 0.6957547\n",
      "________________ 193 0.7060958\n",
      "________________ 194 0.7186139\n",
      "________________ 195 0.72423804\n",
      "________________ 196 0.7411103\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb353937a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb353937a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 197 0.75526124\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb34ba8950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb34ba8950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 198 0.7719521\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb2d4ee320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb2d4ee320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 199 0.7766691\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb34246ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb34246ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 200 0.78120464\n",
      "________________ 201 0.77812046\n",
      "________________ 202 0.7637881\n",
      "________________ 203 0.758164\n",
      "________________ 204 0.72623366\n",
      "________________ 205 0.6933962\n",
      "________________ 206 0.66055876\n",
      "________________ 207 0.6418723\n",
      "________________ 208 0.6242743\n",
      "________________ 209 0.6097605\n",
      "________________ 210 0.6144775\n",
      "________________ 211 0.6159289\n",
      "________________ 212 0.6199202\n",
      "________________ 213 0.6191945\n",
      "________________ 214 0.61810595\n",
      "________________ 215 0.6191945\n",
      "________________ 216 0.6142961\n",
      "________________ 217 0.6081277\n",
      "________________ 218 0.607402\n",
      "________________ 219 0.6170174\n",
      "________________ 220 0.62518144\n",
      "________________ 221 0.62391144\n",
      "________________ 222 0.6264514\n",
      "________________ 223 0.63769954\n",
      "________________ 224 0.65547895\n",
      "________________ 225 0.67035556\n",
      "________________ 226 0.6667271\n",
      "________________ 227 0.6533019\n",
      "________________ 228 0.63824385\n",
      "________________ 229 0.63116837\n",
      "________________ 230 0.62681425\n",
      "________________ 231 0.6230044\n",
      "________________ 232 0.6191945\n",
      "________________ 233 0.61411464\n",
      "________________ 234 0.61411464\n",
      "________________ 235 0.61738026\n",
      "________________ 236 0.62391144\n",
      "________________ 237 0.63261974\n",
      "________________ 238 0.6427794\n",
      "________________ 239 0.6451379\n",
      "________________ 240 0.6471335\n",
      "________________ 241 0.65711176\n",
      "________________ 242 0.6710813\n",
      "________________ 243 0.6681785\n",
      "________________ 244 0.615566\n",
      "________________ 245 0.59687954\n",
      "________________ 246 0.5852685\n",
      "________________ 247 0.58000726\n",
      "________________ 248 0.5803701\n",
      "________________ 249 0.5852685\n",
      "________________ 250 0.59179974\n",
      "________________ 251 0.5994195\n",
      "________________ 252 0.6034107\n",
      "________________ 253 0.612119\n",
      "________________ 254 0.6317126\n",
      "________________ 255 0.6387881\n",
      "________________ 256 0.61103046\n",
      "________________ 257 0.60032654\n",
      "________________ 258 0.5948839\n",
      "________________ 259 0.59778666\n",
      "________________ 260 0.6079463\n",
      "________________ 261 0.6132075\n",
      "________________ 262 0.62119013\n",
      "________________ 263 0.6298984\n",
      "________________ 264 0.6340711\n",
      "________________ 265 0.63588536\n",
      "________________ 266 0.64096516\n",
      "________________ 267 0.6438679\n",
      "________________ 268 0.6453193\n",
      "________________ 269 0.6600145\n",
      "________________ 270 0.6754354\n",
      "________________ 271 0.7198839\n",
      "________________ 272 0.7715893\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb339f9200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb339f9200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 273 0.8046081\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb331293b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb331293b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 274 0.8439768\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb32898680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb32898680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 275 0.8648403\n",
      "________________ 276 0.80061686\n",
      "________________ 277 0.81458634\n",
      "________________ 278 0.8289187\n",
      "________________ 279 0.80805516\n",
      "________________ 280 0.7641509\n",
      "________________ 281 0.7334906\n",
      "________________ 282 0.71317124\n",
      "________________ 283 0.68849784\n",
      "________________ 284 0.67978954\n",
      "________________ 285 0.6899492\n",
      "________________ 286 0.7039187\n",
      "________________ 287 0.71044993\n",
      "________________ 288 0.68214804\n",
      "________________ 289 0.6455007\n",
      "________________ 290 0.61810595\n",
      "________________ 291 0.58635706\n",
      "________________ 292 0.5636792\n",
      "________________ 293 0.55061686\n",
      "________________ 294 0.53592163\n",
      "________________ 295 0.52376634\n",
      "________________ 296 0.51832366\n",
      "________________ 297 0.51560235\n",
      "________________ 298 0.5146952\n",
      "________________ 299 0.51433235\n",
      "________________ 300 0.51396954\n",
      "________________ 301 0.51396954\n",
      "________________ 302 0.51360667\n",
      "________________ 303 0.51360667\n",
      "________________ 304 0.5141509\n",
      "________________ 305 0.5152395\n",
      "________________ 306 0.5170537\n",
      "________________ 307 0.5179608\n",
      "________________ 308 0.5177794\n",
      "________________ 309 0.517598\n",
      "________________ 310 0.517598\n",
      "________________ 311 0.5188679\n",
      "________________ 312 0.5195936\n",
      "________________ 313 0.5212264\n",
      "________________ 314 0.5243106\n",
      "________________ 315 0.52812046\n",
      "________________ 316 0.53447026\n",
      "________________ 317 0.5439042\n",
      "________________ 318 0.5576923\n",
      "________________ 319 0.57674164\n",
      "________________ 320 0.59923804\n",
      "________________ 321 0.6246371\n",
      "________________ 322 0.65384614\n",
      "________________ 323 0.6843251\n",
      "________________ 324 0.7093614\n",
      "________________ 325 0.7398403\n",
      "________________ 326 0.77576196\n",
      "________________ 327 0.774492\n",
      "________________ 328 0.774492\n",
      "________________ 329 0.75362843\n",
      "________________ 330 0.7119013\n",
      "________________ 331 0.67706823\n",
      "________________ 332 0.6471335\n",
      "________________ 333 0.6280842\n",
      "________________ 334 0.61103046\n",
      "________________ 335 0.6064949\n",
      "________________ 336 0.6048621\n",
      "________________ 337 0.6182874\n",
      "________________ 338 0.63661104\n",
      "________________ 339 0.65947026\n",
      "________________ 340 0.67035556\n",
      "________________ 341 0.6857765\n",
      "________________ 342 0.71607405\n",
      "________________ 343 0.7106314\n",
      "________________ 344 0.704463\n",
      "________________ 0 0.7157112\n",
      "________________ 1 0.72895503\n",
      "________________ 2 0.7445573\n",
      "________________ 3 0.7590711\n",
      "________________ 4 0.77304065\n",
      "________________ 5 0.7793904\n",
      "________________ 6 0.78846157\n",
      "________________ 7 0.7928157\n",
      "________________ 8 0.79426706\n",
      "________________ 9 0.79499274\n",
      "________________ 10 0.79499274\n",
      "________________ 11 0.7948113\n",
      "________________ 12 0.7929971\n",
      "________________ 13 0.7848331\n",
      "________________ 14 0.77975327\n",
      "________________ 15 0.7739478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________ 16 0.7708636\n",
      "________________ 17 0.7677794\n",
      "________________ 18 0.75961536\n",
      "________________ 19 0.75417274\n",
      "________________ 20 0.7507257\n",
      "________________ 21 0.7503629\n",
      "________________ 22 0.7556241\n",
      "________________ 23 0.758164\n",
      "________________ 24 0.7645138\n",
      "________________ 25 0.7775762\n",
      "________________ 26 0.7866473\n",
      "________________ 27 0.80206823\n",
      "________________ 28 0.8147678\n",
      "________________ 29 0.8254717\n",
      "________________ 30 0.8321843\n",
      "________________ 31 0.83236575\n",
      "________________ 32 0.82964444\n",
      "________________ 33 0.8298258\n",
      "________________ 34 0.8301887\n",
      "________________ 35 0.8305515\n",
      "________________ 36 0.8258346\n",
      "________________ 37 0.8149492\n",
      "________________ 38 0.79825836\n",
      "________________ 39 0.7837446\n",
      "________________ 40 0.76977503\n",
      "________________ 41 0.7601597\n",
      "________________ 42 0.76269954\n",
      "________________ 43 0.76832366\n",
      "________________ 44 0.77068216\n",
      "________________ 45 0.77576196\n",
      "________________ 46 0.78955007\n",
      "________________ 47 0.79843974\n",
      "________________ 48 0.8124093\n",
      "________________ 49 0.83236575\n",
      "________________ 50 0.8439768\n",
      "________________ 51 0.8447025\n",
      "________________ 52 0.8441582\n",
      "________________ 53 0.845791\n",
      "________________ 54 0.8430697\n",
      "________________ 55 0.8416183\n",
      "________________ 56 0.83508706\n",
      "________________ 57 0.8314586\n",
      "________________ 58 0.82837445\n",
      "________________ 59 0.82529026\n",
      "________________ 60 0.8238389\n",
      "________________ 61 0.8227504\n",
      "________________ 62 0.8223875\n",
      "________________ 63 0.8271045\n",
      "________________ 64 0.82964444\n",
      "________________ 65 0.8372642\n",
      "________________ 66 0.8441582\n",
      "________________ 67 0.845791\n",
      "________________ 68 0.8372642\n",
      "________________ 69 0.83236575\n",
      "________________ 70 0.82293177\n",
      "________________ 71 0.81857765\n",
      "________________ 72 0.8149492\n",
      "________________ 73 0.8187591\n",
      "________________ 74 0.8234761\n",
      "________________ 75 0.828193\n",
      "________________ 76 0.8278302\n",
      "________________ 77 0.8278302\n",
      "________________ 78 0.82601595\n",
      "________________ 79 0.8261974\n",
      "________________ 80 0.82329464\n",
      "________________ 81 0.82093614\n",
      "________________ 82 0.82166183\n",
      "________________ 83 0.82293177\n",
      "________________ 84 0.8191219\n",
      "________________ 85 0.8147678\n",
      "________________ 86 0.8176705\n",
      "________________ 87 0.8220247\n",
      "________________ 88 0.8301887\n",
      "________________ 89 0.8399855\n",
      "________________ 90 0.84851235\n",
      "________________ 91 0.8581277\n",
      "________________ 92 0.86393327\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb31fd59e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb31fd59e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 93 0.8684688\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb319a2cb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb319a2cb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 94 0.87082726\n",
      "________________ 95 0.86973876\n",
      "________________ 96 0.8652032\n",
      "________________ 97 0.86175615\n",
      "________________ 98 0.8597605\n",
      "________________ 99 0.8577649\n",
      "________________ 100 0.8541364\n",
      "________________ 101 0.85195935\n",
      "________________ 102 0.84796804\n",
      "________________ 103 0.8459724\n",
      "________________ 104 0.8427068\n",
      "________________ 105 0.8434325\n",
      "________________ 106 0.84524673\n",
      "________________ 107 0.8514151\n",
      "________________ 108 0.8592163\n",
      "________________ 109 0.8652032\n",
      "________________ 110 0.866836\n",
      "________________ 111 0.8691945\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb31124e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb31124e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 112 0.87445575\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb305710e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb305710e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 113 0.8764514\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb2fbd6200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb2fbd6200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 114 0.8773585\n",
      "________________ 115 0.87608856\n",
      "________________ 116 0.87626994\n",
      "________________ 117 0.87608856\n",
      "________________ 118 0.87608856\n",
      "________________ 119 0.8753629\n",
      "________________ 120 0.8701016\n",
      "________________ 121 0.8597605\n",
      "________________ 122 0.850508\n",
      "________________ 123 0.8412554\n",
      "________________ 124 0.83000726\n",
      "________________ 125 0.8220247\n",
      "________________ 126 0.81349784\n",
      "________________ 127 0.80914366\n",
      "________________ 128 0.80805516\n",
      "________________ 129 0.8100508\n",
      "________________ 130 0.81349784\n",
      "________________ 131 0.81513065\n",
      "________________ 132 0.8173077\n",
      "________________ 133 0.8222061\n",
      "________________ 134 0.8320029\n",
      "________________ 135 0.8432511\n",
      "________________ 136 0.85431784\n",
      "________________ 137 0.8584906\n",
      "________________ 138 0.8637518\n",
      "________________ 139 0.8679245\n",
      "________________ 140 0.8717344\n",
      "________________ 141 0.8740929\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb2fbd6c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb2fbd6c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 142 0.8789913\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb2ee2add0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb2ee2add0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 143 0.88534105\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb2e936f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb2e936f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 144 0.88842523\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb2dcdd170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb2dcdd170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 145 0.8907837\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fead3d53320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fead3d53320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 146 0.89604497\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fead34ab4d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fead34ab4d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 147 0.89640784\n",
      "________________ 148 0.8931422\n",
      "________________ 149 0.8929608\n",
      "________________ 150 0.8929608\n",
      "________________ 151 0.8922351\n",
      "________________ 152 0.8898766\n",
      "________________ 153 0.88661104\n",
      "________________ 154 0.8813498\n",
      "________________ 155 0.87209725\n",
      "________________ 156 0.86411464\n",
      "________________ 157 0.8554064\n",
      "________________ 158 0.84851235\n",
      "________________ 159 0.8427068\n",
      "________________ 160 0.83544993\n",
      "________________ 161 0.8285559\n",
      "________________ 162 0.8211176\n",
      "________________ 163 0.8164006\n",
      "________________ 164 0.8107765\n",
      "________________ 165 0.8069666\n",
      "________________ 166 0.79735124\n",
      "________________ 167 0.78519595\n",
      "________________ 168 0.77304065\n",
      "________________ 169 0.7715893\n",
      "________________ 170 0.77249634\n",
      "________________ 171 0.77467346\n",
      "________________ 172 0.78519595\n",
      "________________ 173 0.8038824\n",
      "________________ 174 0.8171263\n",
      "________________ 175 0.8320029\n",
      "________________ 176 0.84651667\n",
      "________________ 177 0.866836\n",
      "________________ 178 0.8782656\n",
      "________________ 179 0.8895138\n",
      "________________ 180 0.8962264\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fead2fd8680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fead2fd8680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 181 0.9051161\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fead23fc830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fead23fc830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 182 0.9127358\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fead1e369e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fead1e369e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 183 0.91709\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fead118eb90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fead118eb90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 184 0.91763425\n",
      "________________ 185 0.91182876\n",
      "________________ 186 0.9065675\n",
      "________________ 187 0.90221334\n",
      "________________ 188 0.8985849\n",
      "________________ 189 0.89060235\n",
      "________________ 190 0.8871553\n",
      "________________ 191 0.8862482\n",
      "________________ 192 0.8851597\n",
      "________________ 193 0.884434\n",
      "________________ 194 0.883164\n",
      "________________ 195 0.8829826\n",
      "________________ 196 0.8828012\n",
      "________________ 197 0.8837083\n",
      "________________ 198 0.88425255\n",
      "________________ 199 0.883164\n",
      "________________ 200 0.87917274\n",
      "________________ 201 0.8789913\n",
      "________________ 202 0.8798984\n",
      "________________ 203 0.8817126\n",
      "________________ 204 0.89568216\n",
      "________________ 205 0.9040276\n",
      "________________ 206 0.91219157\n",
      "________________ 207 0.91763425\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fead09fdd40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fead09fdd40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 208 0.9178157\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fead0453ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fead0453ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 209 0.91835994\n",
      "________________ 210 0.9179971\n",
      "________________ 211 0.9165457\n",
      "________________ 212 0.91527575\n",
      "________________ 213 0.91600144\n",
      "________________ 214 0.9154572\n",
      "________________ 215 0.9147315\n",
      "________________ 216 0.91418725\n",
      "________________ 217 0.91455007\n",
      "________________ 218 0.9156386\n",
      "________________ 219 0.91582\n",
      "________________ 220 0.9156386\n",
      "________________ 221 0.9156386\n",
      "________________ 222 0.91527575\n",
      "________________ 223 0.9147315\n",
      "________________ 224 0.9156386\n",
      "________________ 225 0.91600144\n",
      "________________ 226 0.9147315\n",
      "________________ 227 0.91418725\n",
      "________________ 228 0.9127358\n",
      "________________ 229 0.91255444\n",
      "________________ 230 0.91182876\n",
      "________________ 231 0.90856314\n",
      "________________ 232 0.90947026\n",
      "________________ 233 0.9103774\n",
      "________________ 234 0.91019595\n",
      "________________ 235 0.90947026\n",
      "________________ 236 0.912373\n",
      "________________ 237 0.91219157\n",
      "________________ 238 0.8953193\n",
      "________________ 239 0.808418\n",
      "________________ 240 0.88769954\n",
      "________________ 241 0.91055876\n",
      "________________ 242 0.9060232\n",
      "________________ 243 0.8971335\n",
      "________________ 244 0.88824385\n",
      "________________ 245 0.883164\n",
      "________________ 246 0.8769956\n",
      "________________ 247 0.87282294\n",
      "________________ 248 0.8699202\n",
      "________________ 249 0.86774313\n",
      "________________ 250 0.871553\n",
      "________________ 251 0.87445575\n",
      "________________ 252 0.8753629\n",
      "________________ 253 0.87554425\n",
      "________________ 254 0.8753629\n",
      "________________ 255 0.8753629\n",
      "________________ 256 0.8746371\n",
      "________________ 257 0.8735486\n",
      "________________ 258 0.8742743\n",
      "________________ 259 0.87445575\n",
      "________________ 260 0.87790275\n",
      "________________ 261 0.87917274\n",
      "________________ 262 0.88352686\n",
      "________________ 263 0.88697386\n",
      "________________ 264 0.8887881\n",
      "________________ 265 0.8920537\n",
      "________________ 266 0.8936865\n",
      "________________ 267 0.8955007\n",
      "________________ 268 0.89604497\n",
      "________________ 269 0.8965893\n",
      "________________ 270 0.8962264\n",
      "________________ 271 0.8969521\n",
      "________________ 272 0.8969521\n",
      "________________ 273 0.8982221\n",
      "________________ 274 0.89912915\n",
      "________________ 275 0.9033019\n",
      "________________ 276 0.90547895\n",
      "________________ 277 0.9072932\n",
      "________________ 278 0.90892595\n",
      "________________ 279 0.9067489\n",
      "________________ 280 0.8982221\n",
      "________________ 281 0.8851597\n",
      "________________ 282 0.87209725\n",
      "________________ 283 0.8572206\n",
      "________________ 284 0.8483309\n",
      "________________ 285 0.8436139\n",
      "________________ 286 0.8427068\n",
      "________________ 287 0.842344\n",
      "________________ 288 0.8142235\n",
      "________________ 289 0.774492\n",
      "________________ 290 0.7358491\n",
      "________________ 291 0.68849784\n",
      "________________ 292 0.65947026\n",
      "________________ 293 0.6427794\n",
      "________________ 294 0.6333454\n",
      "________________ 295 0.63261974\n",
      "________________ 296 0.64060235\n",
      "________________ 297 0.654209\n",
      "________________ 298 0.70772856\n",
      "________________ 299 0.7601597\n",
      "________________ 300 0.80569667\n",
      "________________ 301 0.8425254\n",
      "________________ 302 0.86502177\n",
      "________________ 303 0.8699202\n",
      "________________ 304 0.87137157\n",
      "________________ 305 0.87626994\n",
      "________________ 306 0.8829826\n",
      "________________ 307 0.8953193\n",
      "________________ 308 0.90783745\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb3049e5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feb3049e5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 309 0.9221698\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feacf4148c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feacf4148c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 310 0.9277939\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feacee95b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feacee95b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 311 0.9288824\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feace5c4d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7feace5c4d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "INFO:tensorflow:Assets written to: origCNNpathlolol/assets\n",
      "________________ 312 0.92978954\n",
      "________________ 313 0.92706823\n",
      "________________ 314 0.92561686\n",
      "________________ 315 0.9230769\n",
      "________________ 316 0.9210813\n",
      "________________ 317 0.9210813\n",
      "________________ 318 0.920537\n",
      "________________ 319 0.9179971\n",
      "________________ 320 0.9174528\n",
      "________________ 321 0.91527575\n",
      "________________ 322 0.912373\n",
      "________________ 323 0.9074746\n",
      "________________ 324 0.9063861\n",
      "________________ 325 0.9056604\n",
      "________________ 326 0.9051161\n",
      "________________ 327 0.90620464\n",
      "________________ 328 0.90947026\n",
      "________________ 329 0.912373\n",
      "________________ 330 0.91364294\n",
      "________________ 331 0.91128445\n",
      "________________ 332 0.91364294\n",
      "________________ 333 0.9156386\n",
      "________________ 334 0.91727144\n",
      "________________ 335 0.91709\n",
      "________________ 336 0.91763425\n",
      "________________ 337 0.91709\n",
      "________________ 338 0.9165457\n",
      "________________ 339 0.9150943\n",
      "________________ 340 0.9167271\n",
      "________________ 341 0.9210813\n",
      "________________ 342 0.92089987\n",
      "________________ 343 0.9194485\n",
      "________________ 344 0.9190856\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "# Train\n",
    "train_error_hist = list()\n",
    "test_error_hist = list()\n",
    "test_acc_hist = list()\n",
    "\n",
    "itr = 0\n",
    "n_batches = math.ceil(len(scaled_x_train) / batch_size)\n",
    "\n",
    "\n",
    "ratio_g = 1\n",
    "ratio_b = 1\n",
    "best_model = model\n",
    "best_acc = 0.1\n",
    "best_index = 0\n",
    "savepath = 'origCNNpathlolol'\n",
    "\n",
    "# sample_weight = np.random.rand(len(X_train_g[0]))\n",
    "\n",
    "while itr < 2:\n",
    "    \n",
    "    ptr_g = 0\n",
    "    ptr_b = 0\n",
    "\n",
    "    allot_num_g = 1\n",
    "    allot_num_b = 1\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        \n",
    "        allot_num_g = ( ratio_g / (ratio_g + ratio_b) )*batch_size\n",
    "        allot_num_g = (int)(allot_num_g)\n",
    "        allot_num_b = batch_size - allot_num_g\n",
    "\n",
    "        X_batch = sx_train_g[ ptr_g: ptr_g+allot_num_g ]\n",
    "        tempp = sx_train_b[ ptr_b: ptr_b+allot_num_b]\n",
    "        #         print(\"   \", len(X_batch), len(tempp))\n",
    "        y_batch = y_train_g[ ptr_g: ptr_g+allot_num_g]\n",
    "        tempp1 = y_train_b[ ptr_b: ptr_b+allot_num_b]\n",
    "\n",
    "        if len(X_batch)==0:\n",
    "            ptr_g=0\n",
    "        elif len(tempp)==0:\n",
    "            ptr_b=0\n",
    "        X_batch = np.concatenate([X_batch, tempp])        \n",
    "        y_batch = np.concatenate([y_batch, tempp1])\n",
    "        loss = model.train_on_batch(X_batch, y_batch)\n",
    "\n",
    "        train_error_hist.append(loss[0])\n",
    "        \n",
    "        loss1 = model.test_on_batch(scaled_x_test, y_test)\n",
    "        test_error_hist.append(loss1[0])\n",
    "        test_acc_hist.append(loss1[1])\n",
    "        if loss1[1] > best_acc and loss1[1] > 0.75:\n",
    "            best_acc = loss1[1]\n",
    "            model.save(savepath)\n",
    "            best_model = keras.models.load_model(savepath)\n",
    "        print(\"________________\",i, loss1[1])\n",
    "        ratio_g, ratio_b = eval_ratios(sx_train_g[:1024], sx_train_b[:1024], y_train_g[:1024], y_train_b[:1024], model)\n",
    "#         ratio_g = 1\n",
    "#         ratio_b = 1\n",
    "#         print(ratio_g, ratio_b)\n",
    "        ptr_b+=allot_num_b\n",
    "        ptr_g+=allot_num_g\n",
    "    \n",
    "    itr = itr+1\n",
    "\n",
    "# model.fit(x=scaled_x_train, y=y_train, epochs=iterations, validation_split=validation_split, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " End:  2022-05-10 12:22:49.475788\n",
      "0.92978954\n",
      "0.9190856\n"
     ]
    }
   ],
   "source": [
    "print(\" End: \", datetime.now())\n",
    "print(max(test_acc_hist))\n",
    "print(test_acc_hist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# history = model.fit(np.array(X_train), \n",
    "#                          y_train, \n",
    "#                          batch_size = 64, \n",
    "#                          verbose = 1, \n",
    "#                          epochs = 5,      #Changed to 3 from 50 for testing purposes.\n",
    "#                          validation_split = 0.1,\n",
    "#                          shuffle = False\n",
    "#                       #   callbacks=callbacks\n",
    "#                      )\n",
    "\n",
    "# history = model.train_on_batch(np.array(X_train[:128]),y_train[:128])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\\nfrom keras.models import Sequential\\nmodel = None\\nmodel = Sequential()\\nmodel.add(Convolution2D(32, (3, 3), input_shape = (SIZE, SIZE, 3), activation = \\'relu\\', data_format=\\'channels_last\\'))\\nmodel.add(MaxPooling2D(pool_size = (2, 2), data_format=\"channels_last\"))\\nmodel.add(BatchNormalization(axis = -1))\\nmodel.add(Dropout(0.2))\\nmodel.add(Convolution2D(32, (3, 3), activation = \\'relu\\'))\\nmodel.add(MaxPooling2D(pool_size = (2, 2), data_format=\"channels_last\"))\\nmodel.add(BatchNormalization(axis = -1))\\nmodel.add(Dropout(0.2))\\nmodel.add(Flatten())\\nmodel.add(Dense(activation = \\'relu\\', units=512))\\nmodel.add(BatchNormalization(axis = -1))\\nmodel.add(Dropout(0.2))\\nmodel.add(Dense(activation = \\'relu\\', units=256))\\nmodel.add(BatchNormalization(axis = -1))\\nmodel.add(Dropout(0.2))\\nmodel.add(Dense(activation = \\'sigmoid\\', units=2))\\nmodel.compile(optimizer = \\'adam\\', loss = \\'categorical_crossentropy\\', metrics = [\\'accuracy\\'])\\nprint(model.summary())\\n'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ## Accuracy calculationany\n",
    "# \n",
    "# I'll now calculate the accuracy on the test data.\n",
    "plt.plot(test_acc_hist)\n",
    "# print(\"Test_Accuracy: {:.2f}%\".format(model.evaluate(np.array(X_test), np.array(y_test))[1]*100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "# t = f.suptitle('CNN Performance', fontsize=12)\n",
    "# f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "# max_epoch = len(history.history['accuracy'])+1\n",
    "# epoch_list = list(range(1,max_epoch))\n",
    "# ax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\n",
    "# ax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# ax1.set_xticks(np.arange(1, max_epoch, 5))\n",
    "# ax1.set_ylabel('Accuracy Value')\n",
    "# ax1.set_xlabel('Epoch')\n",
    "# ax1.set_title('Accuracy')\n",
    "# l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "# ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n",
    "# ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n",
    "# ax2.set_xticks(np.arange(1, max_epoch, 5))\n",
    "# ax2.set_ylabel('Loss Value')\n",
    "# ax2.set_xlabel('Epoch')\n",
    "# ax2.set_title('Loss')\n",
    "# l2 = ax2.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "#Save the model\n",
    "# model.save('malaria_cnn.h5')\n",
    "\n",
    "################################################\n",
    "### ANOTHER WAY TO DEFINE THE NETWORK using Sequential model\n",
    "#Sequential \n",
    "#You can create a Sequential model by passing a list of layer instances to the constructor:\n",
    "\"\"\"\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "model = None\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, (3, 3), input_shape = (SIZE, SIZE, 3), activation = 'relu', data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2), data_format=\"channels_last\"))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution2D(32, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2), data_format=\"channels_last\"))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(activation = 'relu', units=512))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(activation = 'relu', units=256))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(activation = 'sigmoid', units=2))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.94      0.93      2703\n",
      "         1.0       0.94      0.92      0.93      2809\n",
      "\n",
      "    accuracy                           0.93      5512\n",
      "   macro avg       0.93      0.93      0.93      5512\n",
      "weighted avg       0.93      0.93      0.93      5512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "# # with as\n",
    "predictions = best_model.predict(scaled_x_test)\n",
    "predictions=np.argmax(predictions,axis=1)\n",
    "print(classification_report(y_test.T[1], predictions))\n",
    "\n",
    "# print(y_test.T[1],predictions)\n",
    "# for i in range(100):\n",
    "#     print(predictions[i],y_test.T[1][i] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.92      0.93      2809\n",
      "         1.0       0.92      0.94      0.93      2703\n",
      "\n",
      "    accuracy                           0.93      5512\n",
      "   macro avg       0.93      0.93      0.93      5512\n",
      "weighted avg       0.93      0.93      0.93      5512\n",
      "\n",
      "[[2590  219]\n",
      " [ 168 2535]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def myfunc(x):\n",
    "    if x[0] > x[1]:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "y_pred = best_model.predict(scaled_x_test)\n",
    "# print(yas_pred)\n",
    "# yas_pred = np.argmax(yas_pred)\n",
    "# print(yas_pred)\n",
    "\n",
    "# print(y_test)\n",
    "for i in range(len(y_pred)):\n",
    "#     print(y_pred[i])\n",
    "    y_pred[i] = myfunc(y_pred[i])\n",
    "    y_test[i] = myfunc(y_test[i])\n",
    "# print(y_pred)\n",
    "print(classification_report(y_test.T[0], y_pred.T[0]))\n",
    "# print(y_test, y_pred)\n",
    "print(confusion_matrix(y_test.T[0], y_pred.T[0]))\n",
    "# print(y_pred.T[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.training.Model object at 0x7feac9b2ffd0>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.saving.saved_model.load.Model object at 0x7feace0e0790>\n"
     ]
    }
   ],
   "source": [
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5097968, 0.50961536, 0.51342523, 0.5883527, 0.5676705, 0.49709725, 0.49056605, 0.4902032, 0.4903846, 0.4903846, 0.49056605, 0.4903846, 0.4909289, 0.49129173, 0.51560235, 0.5569666, 0.5562409, 0.5576923, 0.56222785, 0.5703919, 0.58581275, 0.5821843, 0.57529026, 0.5660377, 0.55805516, 0.55079824, 0.54100144, 0.53610307, 0.5330189, 0.5324746, 0.532656, 0.53610307, 0.53846157, 0.54698837, 0.554971, 0.56313497, 0.5769231, 0.5867199, 0.5934325, 0.5990566, 0.6101234, 0.6171988, 0.62681425, 0.6367925, 0.6418723, 0.66182876, 0.6587446, 0.64985484, 0.6253629, 0.61338896, 0.59887516, 0.5941582, 0.5872642, 0.5845428, 0.58291, 0.5769231, 0.5683962, 0.55986935, 0.55370104, 0.54970974, 0.55079824, 0.5544267, 0.5689405, 0.58309144, 0.5883527, 0.5916183, 0.5974238, 0.6021408, 0.6034107, 0.60268503, 0.60867196, 0.6222787, 0.6306241, 0.6415094, 0.64840347, 0.64677066, 0.6485849, 0.6543904, 0.65457183, 0.64731497, 0.63661104, 0.63751817, 0.6362482, 0.64876634, 0.6565675, 0.65312046, 0.6317126, 0.6166546, 0.6068578, 0.5963353, 0.5927068, 0.5903483, 0.5907112, 0.5932511, 0.5896226, 0.5950653, 0.59379536, 0.60123366, 0.6063135, 0.615566, 0.620283, 0.620283, 0.62880987, 0.6387881, 0.64495647, 0.63824385, 0.63225687, 0.6407837, 0.6533019, 0.6563861, 0.6572932, 0.65820026, 0.6371553, 0.61175615, 0.59452105, 0.5932511, 0.58744556, 0.5838171, 0.58780843, 0.5981495, 0.62318575, 0.6469521, 0.66418725, 0.64241654, 0.5990566, 0.5774673, 0.56513065, 0.5609579, 0.5562409, 0.56113935, 0.55206823, 0.5495283, 0.54426706, 0.5446299, 0.5446299, 0.5464441, 0.55079824, 0.5544267, 0.5578737, 0.5694848, 0.5683962, 0.570029, 0.5660377, 0.5680334, 0.574746, 0.5814586, 0.58944124, 0.62681425, 0.66491294, 0.6857765, 0.6937591, 0.690312, 0.68414366, 0.66110307, 0.6380624, 0.6333454, 0.62481856, 0.6044993, 0.5925254, 0.5959724, 0.6014151, 0.6023222, 0.6041364, 0.6041364, 0.6159289, 0.64060235, 0.6835994, 0.7162554, 0.7249637, 0.7247823, 0.71317124, 0.6714441, 0.6357039, 0.60159653, 0.5908926, 0.58472425, 0.58581275, 0.5907112, 0.59452105, 0.6021408, 0.6068578, 0.61738026, 0.6191945, 0.625, 0.6347968, 0.66110307, 0.6850508, 0.7086357, 0.7298621, 0.7213353, 0.71389693, 0.6984761, 0.6957547, 0.7060958, 0.7186139, 0.72423804, 0.7411103, 0.75526124, 0.7719521, 0.7766691, 0.78120464, 0.77812046, 0.7637881, 0.758164, 0.72623366, 0.6933962, 0.66055876, 0.6418723, 0.6242743, 0.6097605, 0.6144775, 0.6159289, 0.6199202, 0.6191945, 0.61810595, 0.6191945, 0.6142961, 0.6081277, 0.607402, 0.6170174, 0.62518144, 0.62391144, 0.6264514, 0.63769954, 0.65547895, 0.67035556, 0.6667271, 0.6533019, 0.63824385, 0.63116837, 0.62681425, 0.6230044, 0.6191945, 0.61411464, 0.61411464, 0.61738026, 0.62391144, 0.63261974, 0.6427794, 0.6451379, 0.6471335, 0.65711176, 0.6710813, 0.6681785, 0.615566, 0.59687954, 0.5852685, 0.58000726, 0.5803701, 0.5852685, 0.59179974, 0.5994195, 0.6034107, 0.612119, 0.6317126, 0.6387881, 0.61103046, 0.60032654, 0.5948839, 0.59778666, 0.6079463, 0.6132075, 0.62119013, 0.6298984, 0.6340711, 0.63588536, 0.64096516, 0.6438679, 0.6453193, 0.6600145, 0.6754354, 0.7198839, 0.7715893, 0.8046081, 0.8439768, 0.8648403, 0.80061686, 0.81458634, 0.8289187, 0.80805516, 0.7641509, 0.7334906, 0.71317124, 0.68849784, 0.67978954, 0.6899492, 0.7039187, 0.71044993, 0.68214804, 0.6455007, 0.61810595, 0.58635706, 0.5636792, 0.55061686, 0.53592163, 0.52376634, 0.51832366, 0.51560235, 0.5146952, 0.51433235, 0.51396954, 0.51396954, 0.51360667, 0.51360667, 0.5141509, 0.5152395, 0.5170537, 0.5179608, 0.5177794, 0.517598, 0.517598, 0.5188679, 0.5195936, 0.5212264, 0.5243106, 0.52812046, 0.53447026, 0.5439042, 0.5576923, 0.57674164, 0.59923804, 0.6246371, 0.65384614, 0.6843251, 0.7093614, 0.7398403, 0.77576196, 0.774492, 0.774492, 0.75362843, 0.7119013, 0.67706823, 0.6471335, 0.6280842, 0.61103046, 0.6064949, 0.6048621, 0.6182874, 0.63661104, 0.65947026, 0.67035556, 0.6857765, 0.71607405, 0.7106314, 0.704463, 0.7157112, 0.72895503, 0.7445573, 0.7590711, 0.77304065, 0.7793904, 0.78846157, 0.7928157, 0.79426706, 0.79499274, 0.79499274, 0.7948113, 0.7929971, 0.7848331, 0.77975327, 0.7739478, 0.7708636, 0.7677794, 0.75961536, 0.75417274, 0.7507257, 0.7503629, 0.7556241, 0.758164, 0.7645138, 0.7775762, 0.7866473, 0.80206823, 0.8147678, 0.8254717, 0.8321843, 0.83236575, 0.82964444, 0.8298258, 0.8301887, 0.8305515, 0.8258346, 0.8149492, 0.79825836, 0.7837446, 0.76977503, 0.7601597, 0.76269954, 0.76832366, 0.77068216, 0.77576196, 0.78955007, 0.79843974, 0.8124093, 0.83236575, 0.8439768, 0.8447025, 0.8441582, 0.845791, 0.8430697, 0.8416183, 0.83508706, 0.8314586, 0.82837445, 0.82529026, 0.8238389, 0.8227504, 0.8223875, 0.8271045, 0.82964444, 0.8372642, 0.8441582, 0.845791, 0.8372642, 0.83236575, 0.82293177, 0.81857765, 0.8149492, 0.8187591, 0.8234761, 0.828193, 0.8278302, 0.8278302, 0.82601595, 0.8261974, 0.82329464, 0.82093614, 0.82166183, 0.82293177, 0.8191219, 0.8147678, 0.8176705, 0.8220247, 0.8301887, 0.8399855, 0.84851235, 0.8581277, 0.86393327, 0.8684688, 0.87082726, 0.86973876, 0.8652032, 0.86175615, 0.8597605, 0.8577649, 0.8541364, 0.85195935, 0.84796804, 0.8459724, 0.8427068, 0.8434325, 0.84524673, 0.8514151, 0.8592163, 0.8652032, 0.866836, 0.8691945, 0.87445575, 0.8764514, 0.8773585, 0.87608856, 0.87626994, 0.87608856, 0.87608856, 0.8753629, 0.8701016, 0.8597605, 0.850508, 0.8412554, 0.83000726, 0.8220247, 0.81349784, 0.80914366, 0.80805516, 0.8100508, 0.81349784, 0.81513065, 0.8173077, 0.8222061, 0.8320029, 0.8432511, 0.85431784, 0.8584906, 0.8637518, 0.8679245, 0.8717344, 0.8740929, 0.8789913, 0.88534105, 0.88842523, 0.8907837, 0.89604497, 0.89640784, 0.8931422, 0.8929608, 0.8929608, 0.8922351, 0.8898766, 0.88661104, 0.8813498, 0.87209725, 0.86411464, 0.8554064, 0.84851235, 0.8427068, 0.83544993, 0.8285559, 0.8211176, 0.8164006, 0.8107765, 0.8069666, 0.79735124, 0.78519595, 0.77304065, 0.7715893, 0.77249634, 0.77467346, 0.78519595, 0.8038824, 0.8171263, 0.8320029, 0.84651667, 0.866836, 0.8782656, 0.8895138, 0.8962264, 0.9051161, 0.9127358, 0.91709, 0.91763425, 0.91182876, 0.9065675, 0.90221334, 0.8985849, 0.89060235, 0.8871553, 0.8862482, 0.8851597, 0.884434, 0.883164, 0.8829826, 0.8828012, 0.8837083, 0.88425255, 0.883164, 0.87917274, 0.8789913, 0.8798984, 0.8817126, 0.89568216, 0.9040276, 0.91219157, 0.91763425, 0.9178157, 0.91835994, 0.9179971, 0.9165457, 0.91527575, 0.91600144, 0.9154572, 0.9147315, 0.91418725, 0.91455007, 0.9156386, 0.91582, 0.9156386, 0.9156386, 0.91527575, 0.9147315, 0.9156386, 0.91600144, 0.9147315, 0.91418725, 0.9127358, 0.91255444, 0.91182876, 0.90856314, 0.90947026, 0.9103774, 0.91019595, 0.90947026, 0.912373, 0.91219157, 0.8953193, 0.808418, 0.88769954, 0.91055876, 0.9060232, 0.8971335, 0.88824385, 0.883164, 0.8769956, 0.87282294, 0.8699202, 0.86774313, 0.871553, 0.87445575, 0.8753629, 0.87554425, 0.8753629, 0.8753629, 0.8746371, 0.8735486, 0.8742743, 0.87445575, 0.87790275, 0.87917274, 0.88352686, 0.88697386, 0.8887881, 0.8920537, 0.8936865, 0.8955007, 0.89604497, 0.8965893, 0.8962264, 0.8969521, 0.8969521, 0.8982221, 0.89912915, 0.9033019, 0.90547895, 0.9072932, 0.90892595, 0.9067489, 0.8982221, 0.8851597, 0.87209725, 0.8572206, 0.8483309, 0.8436139, 0.8427068, 0.842344, 0.8142235, 0.774492, 0.7358491, 0.68849784, 0.65947026, 0.6427794, 0.6333454, 0.63261974, 0.64060235, 0.654209, 0.70772856, 0.7601597, 0.80569667, 0.8425254, 0.86502177, 0.8699202, 0.87137157, 0.87626994, 0.8829826, 0.8953193, 0.90783745, 0.9221698, 0.9277939, 0.9288824, 0.92978954, 0.92706823, 0.92561686, 0.9230769, 0.9210813, 0.9210813, 0.920537, 0.9179971, 0.9174528, 0.91527575, 0.912373, 0.9074746, 0.9063861, 0.9056604, 0.9051161, 0.90620464, 0.90947026, 0.912373, 0.91364294, 0.91128445, 0.91364294, 0.9156386, 0.91727144, 0.91709, 0.91763425, 0.91709, 0.9165457, 0.9150943, 0.9167271, 0.9210813, 0.92089987, 0.9194485, 0.9190856]\n"
     ]
    }
   ],
   "source": [
    "print(test_acc_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sriphaniKernel",
   "language": "python",
   "name": "sriphanikernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
